{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available:cpu\n",
      "CUDA Device Name: No GPU detected\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration Cell ---\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Is CUDA available:{device}\")\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "\n",
    "# --- Paths and Constants ---\n",
    "RATINGS_CSV_PATH = 'cleaned_merged.csv' \n",
    "USER_COL = 'UserId'\n",
    "ITEM_COL = 'ProductId'\n",
    "RATING_COL = 'Score'\n",
    "OUTPUT_MODEL_PATH = 'cf_model.pth'\n",
    "CF_MAPPINGS_PATH = 'cf_mappings.pkl' # File to save the new mappings\n",
    "\n",
    "# --- Model and Training Hyperparameters ---\n",
    "EMBEDDING_DIM = 512\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100 \n",
    "TEST_SPLIT = 0.2\n",
    "VALIDATION_SPLIT = 0.2 # Proportion of original data for validation\n",
    "RANDOM_STATE = 42\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Class ---\n",
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, dataframe, user_map, item_map, user_col, item_col, rating_col):\n",
    "        # Mappings are now passed in, derived from the full dataset used for training\n",
    "        # We assume the dataframe passed here only contains users/items present in the maps\n",
    "        self.users = torch.tensor([user_map[i] for i in dataframe[user_col]], dtype=torch.long)\n",
    "        self.items = torch.tensor([item_map[i] for i in dataframe[item_col]], dtype=torch.long)\n",
    "        self.ratings = torch.tensor(dataframe[rating_col].values, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sentiment_label(score):\n",
    "#     if score < 3:\n",
    "#         return 0  # Negative\n",
    "#     elif score == 3:\n",
    "#         return 1  # Neutral\n",
    "#     else:\n",
    "#         return 2  # Positive\n",
    "# # Assuming df_loaded['mean_score'] is the correct column\n",
    "# df_loaded['label'] = df_loaded['mean_score'].apply(sentiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ratings data from cleaned_merged.csv...\n",
      "Loaded 393560 valid ratings.\n",
      "Creating user and item mappings...\n",
      "Found 256027 unique users and 67553 unique items in the dataset.\n",
      "Splitting data into train, validation (20%), and test (20%)...\n",
      "Training set size: 236136\n",
      "Validation set size: 78712\n",
      "Test set size: 78712\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data, Create Mappings, and Split ---\n",
    "\n",
    "user_id_to_idx = {}\n",
    "item_id_to_idx = {}\n",
    "num_users = 0\n",
    "num_items = 0\n",
    "train_loader, val_loader, test_loader = None, None, None\n",
    "\n",
    "print(f\"Loading ratings data from {RATINGS_CSV_PATH}...\")\n",
    "try:\n",
    "    # Load only necessary columns\n",
    "    ratings_df = pd.read_csv(RATINGS_CSV_PATH, usecols=[USER_COL, ITEM_COL, RATING_COL])\n",
    "    ratings_df.dropna(subset=[USER_COL, ITEM_COL, RATING_COL], inplace=True)\n",
    "    print(f\"Loaded {len(ratings_df)} valid ratings.\")\n",
    "\n",
    "    if len(ratings_df) > 0:\n",
    "        # --- Create Mappings Dynamically ---\n",
    "        print(\"Creating user and item mappings...\")\n",
    "        unique_users = sorted(ratings_df[USER_COL].unique().tolist())\n",
    "        unique_items = sorted(ratings_df[ITEM_COL].unique().tolist())\n",
    "\n",
    "        user_id_to_idx = {user_id: idx for idx, user_id in enumerate(unique_users)}\n",
    "        item_id_to_idx = {item_id: idx for idx, item_id in enumerate(unique_items)}\n",
    "\n",
    "        num_users = len(user_id_to_idx)\n",
    "        num_items = len(item_id_to_idx)\n",
    "        print(f\"Found {num_users} unique users and {num_items} unique items in the dataset.\")\n",
    "\n",
    "        # --- Split Data ---\n",
    "        print(f\"Splitting data into train, validation ({VALIDATION_SPLIT:.0%}), and test ({TEST_SPLIT:.0%})...\")\n",
    "        \n",
    "        # First split: Separate test set\n",
    "        train_val_df, test_df = train_test_split(\n",
    "            ratings_df, \n",
    "            test_size=TEST_SPLIT, \n",
    "            random_state=RANDOM_STATE,\n",
    "            # Stratify might be useful if ratings are imbalanced, but requires ratings column\n",
    "            # stratify=ratings_df[RATING_COL] if RATING_COL in ratings_df else None \n",
    "        )\n",
    "        \n",
    "        # Check if train_val_df is empty before the second split\n",
    "        if len(train_val_df) > 0:\n",
    "            # Calculate adjusted validation split size relative to the remaining data\n",
    "            val_split_adjusted = VALIDATION_SPLIT / (1.0 - TEST_SPLIT)\n",
    "            val_split_adjusted = min(val_split_adjusted, 1.0) # Ensure it doesn't exceed 1.0\n",
    "            \n",
    "            train_df, val_df = train_test_split(\n",
    "                train_val_df, \n",
    "                test_size=val_split_adjusted, \n",
    "                random_state=RANDOM_STATE, # Use same random state for consistency\n",
    "                # stratify=train_val_df[RATING_COL] if RATING_COL in train_val_df else None\n",
    "            )\n",
    "            \n",
    "            print(f\"Training set size: {len(train_df)}\")\n",
    "            print(f\"Validation set size: {len(val_df)}\")\n",
    "            print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "            # --- Create Datasets and DataLoaders ---\n",
    "            if len(train_df) > 0:\n",
    "                train_dataset = RatingsDataset(train_df, user_id_to_idx, item_id_to_idx, USER_COL, ITEM_COL, RATING_COL)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0) # num_workers=0 for Windows compatibility often\n",
    "            else:\n",
    "                print(\"Warning: Training set is empty after split.\")\n",
    "                \n",
    "            if len(val_df) > 0:\n",
    "                val_dataset = RatingsDataset(val_df, user_id_to_idx, item_id_to_idx, USER_COL, ITEM_COL, RATING_COL)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "            else:\n",
    "                 print(\"Warning: Validation set is empty after split.\")\n",
    "\n",
    "            if len(test_df) > 0:\n",
    "                test_dataset = RatingsDataset(test_df, user_id_to_idx, item_id_to_idx, USER_COL, ITEM_COL, RATING_COL)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "            else:\n",
    "                print(\"Warning: Test set is empty after split.\")\n",
    "                \n",
    "        else:\n",
    "            print(\"Warning: No data left for training/validation after initial test split.\")\n",
    "            # Handle test set if it's the only one with data\n",
    "            if len(test_df) > 0:\n",
    "                 print(f\"Test set size: {len(test_df)}\")\n",
    "                 test_dataset = RatingsDataset(test_df, user_id_to_idx, item_id_to_idx, USER_COL, ITEM_COL, RATING_COL)\n",
    "                 test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "            else:\n",
    "                 print(\"Warning: All sets are empty after splits.\")\n",
    "    else:\n",
    "        print(\"Error: No valid ratings data loaded. Cannot proceed.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Ratings file not found at {RATINGS_CSV_PATH}\")\n",
    "    raise\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Column {e} not found in {RATINGS_CSV_PATH}. Adjust column names in config.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading, mapping, or splitting: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define CF Model (Matrix Factorization) ---\n",
    "# !! Make sure to run this cell BEFORE the model initialization cell !!\n",
    "import torch.nn as nn # Ensure nn is imported\n",
    "\n",
    "class CFModel(nn.Module):\n",
    "    def __init__(self, n_users, n_items, embedding_dim):\n",
    "        super().__init__()\n",
    "        # Add small epsilon to handle potential zero users/items if data loading failed\n",
    "        self.user_embeddings = nn.Embedding(max(n_users, 1), embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(max(n_items, 1), embedding_dim)\n",
    "        self.user_bias = nn.Embedding(max(n_users, 1), 1)\n",
    "        self.item_bias = nn.Embedding(max(n_items, 1), 1)\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.normal_(self.user_embeddings.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embeddings.weight, std=0.01)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.user_embeddings(user_indices)\n",
    "        item_embedding = self.item_embeddings(item_indices)\n",
    "        dot_product = (user_embedding * item_embedding).sum(1)\n",
    "        user_b = self.user_bias(user_indices).squeeze()\n",
    "        item_b = self.item_bias(item_indices).squeeze()\n",
    "        # Ensure biases are added correctly even if batch size is 1\n",
    "        if dot_product.dim() == 0: # Handle batch size of 1 where squeeze might remove the dim\n",
    "             user_b = user_b.unsqueeze(0)\n",
    "             item_b = item_b.unsqueeze(0)\n",
    "             dot_product = dot_product.unsqueeze(0)\n",
    "        elif user_b.dim() == 0:\n",
    "             user_b = user_b.unsqueeze(0)\n",
    "        elif item_b.dim() == 0:\n",
    "             item_b = item_b.unsqueeze(0)\n",
    "             \n",
    "        # Clamp output to reasonable rating range (e.g., 1-5) if applicable\n",
    "        # return torch.clamp(dot_product + user_b + item_b, 1.0, 5.0) \n",
    "        # Or return raw scores if clamping is not desired\n",
    "        return dot_product + user_b + item_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training and Evaluation Functions ---\n",
    "# !! Make sure to run this cell BEFORE the model initialization/training cell !!\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train() # Set model to training mode\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for users, items, ratings in dataloader:\n",
    "        users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(users, items)\n",
    "        loss = criterion(outputs, ratings)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * users.size(0)\n",
    "        total_samples += users.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples if total_samples > 0 else 0\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        for users, items, ratings in dataloader:\n",
    "            users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(users, items)\n",
    "            loss = criterion(outputs, ratings)\n",
    "\n",
    "            total_loss += loss.item() * users.size(0)\n",
    "            total_samples += users.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples if total_samples > 0 else 0\n",
    "    rmse = math.sqrt(avg_loss) if avg_loss >= 0 else float('nan') # Calculate RMSE from MSE\n",
    "    return avg_loss, rmse\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
    "    best_val_rmse = float('inf')\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- Training ---\n",
    "        avg_train_loss = 0\n",
    "        if train_loader:\n",
    "            avg_train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            train_rmse = math.sqrt(avg_train_loss) if avg_train_loss >= 0 else float('nan')\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Skipping training: train_loader not available.\")\n",
    "            train_rmse = float('nan')\n",
    "\n",
    "        # --- Validation ---\n",
    "        avg_val_loss = float('inf')\n",
    "        val_rmse = float('inf')\n",
    "        if val_loader:\n",
    "            avg_val_loss, val_rmse = evaluate_model(model, val_loader, criterion, device)\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Skipping validation: val_loader not available.\")\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train RMSE: {train_rmse:.4f}, Val Loss: {avg_val_loss:.4f}, Val RMSE: {val_rmse:.4f}')\n",
    "\n",
    "        # --- Checkpoint Best Model (based on validation RMSE) ---\n",
    "        if val_loader and val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_epoch = epoch + 1\n",
    "            # Optionally save the best model checkpoint here if desired\n",
    "            # torch.save(model.state_dict(), 'best_cf_model.pth')\n",
    "            # print(f'   -> New best validation RMSE found: {best_val_rmse:.4f}. Model checkpoint saved.')\n",
    "\n",
    "    print(f'\\nFinished Training. Best Validation RMSE: {best_val_rmse:.4f} at epoch {best_epoch}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF model, criterion, and optimizer initialized.\n",
      "Starting training and validation...\n",
      "Epoch 1/100, Train Loss: 18.4959, Train RMSE: 4.3007, Val Loss: 17.5757, Val RMSE: 4.1923\n",
      "Epoch 2/100, Train Loss: 13.2468, Train RMSE: 3.6396, Val Loss: 16.3076, Val RMSE: 4.0383\n",
      "Epoch 3/100, Train Loss: 4.6678, Train RMSE: 2.1605, Val Loss: 15.9258, Val RMSE: 3.9907\n",
      "Epoch 4/100, Train Loss: 1.1691, Train RMSE: 1.0813, Val Loss: 15.8917, Val RMSE: 3.9864\n",
      "Epoch 5/100, Train Loss: 0.3644, Train RMSE: 0.6037, Val Loss: 15.8405, Val RMSE: 3.9800\n",
      "Epoch 6/100, Train Loss: 0.3176, Train RMSE: 0.5635, Val Loss: 15.7657, Val RMSE: 3.9706\n",
      "Epoch 7/100, Train Loss: 0.5395, Train RMSE: 0.7345, Val Loss: 15.6808, Val RMSE: 3.9599\n",
      "Epoch 8/100, Train Loss: 0.7393, Train RMSE: 0.8598, Val Loss: 15.5495, Val RMSE: 3.9433\n",
      "Epoch 9/100, Train Loss: 0.6887, Train RMSE: 0.8299, Val Loss: 15.4472, Val RMSE: 3.9303\n",
      "Epoch 10/100, Train Loss: 0.5283, Train RMSE: 0.7269, Val Loss: 15.3369, Val RMSE: 3.9162\n",
      "Epoch 11/100, Train Loss: 0.5194, Train RMSE: 0.7207, Val Loss: 15.2582, Val RMSE: 3.9062\n",
      "Epoch 12/100, Train Loss: 0.5221, Train RMSE: 0.7226, Val Loss: 15.1490, Val RMSE: 3.8922\n",
      "Epoch 13/100, Train Loss: 0.5469, Train RMSE: 0.7395, Val Loss: 15.0662, Val RMSE: 3.8815\n",
      "Epoch 14/100, Train Loss: 0.5111, Train RMSE: 0.7149, Val Loss: 14.9599, Val RMSE: 3.8678\n",
      "Epoch 15/100, Train Loss: 0.5122, Train RMSE: 0.7157, Val Loss: 14.8820, Val RMSE: 3.8577\n",
      "Epoch 16/100, Train Loss: 0.4829, Train RMSE: 0.6949, Val Loss: 14.7801, Val RMSE: 3.8445\n",
      "Epoch 17/100, Train Loss: 0.4927, Train RMSE: 0.7019, Val Loss: 14.7058, Val RMSE: 3.8348\n",
      "Epoch 18/100, Train Loss: 0.4656, Train RMSE: 0.6823, Val Loss: 14.6133, Val RMSE: 3.8227\n",
      "Epoch 19/100, Train Loss: 0.4785, Train RMSE: 0.6917, Val Loss: 14.5379, Val RMSE: 3.8129\n",
      "Epoch 20/100, Train Loss: 0.4509, Train RMSE: 0.6715, Val Loss: 14.4507, Val RMSE: 3.8014\n",
      "Epoch 21/100, Train Loss: 0.4629, Train RMSE: 0.6804, Val Loss: 14.3775, Val RMSE: 3.7918\n",
      "Epoch 22/100, Train Loss: 0.4354, Train RMSE: 0.6598, Val Loss: 14.2942, Val RMSE: 3.7808\n",
      "Epoch 23/100, Train Loss: 0.4471, Train RMSE: 0.6686, Val Loss: 14.2240, Val RMSE: 3.7715\n",
      "Epoch 24/100, Train Loss: 0.4239, Train RMSE: 0.6511, Val Loss: 14.1454, Val RMSE: 3.7610\n",
      "Epoch 25/100, Train Loss: 0.4349, Train RMSE: 0.6595, Val Loss: 14.0771, Val RMSE: 3.7519\n",
      "Epoch 26/100, Train Loss: 0.4122, Train RMSE: 0.6421, Val Loss: 14.0027, Val RMSE: 3.7420\n",
      "Epoch 27/100, Train Loss: 0.4233, Train RMSE: 0.6506, Val Loss: 13.9380, Val RMSE: 3.7334\n",
      "Epoch 28/100, Train Loss: 0.4030, Train RMSE: 0.6348, Val Loss: 13.8626, Val RMSE: 3.7232\n",
      "Epoch 29/100, Train Loss: 0.4134, Train RMSE: 0.6429, Val Loss: 13.7999, Val RMSE: 3.7148\n",
      "Epoch 30/100, Train Loss: 0.3895, Train RMSE: 0.6241, Val Loss: 13.7346, Val RMSE: 3.7060\n",
      "Epoch 31/100, Train Loss: 0.4035, Train RMSE: 0.6352, Val Loss: 13.6670, Val RMSE: 3.6969\n",
      "Epoch 32/100, Train Loss: 0.3837, Train RMSE: 0.6194, Val Loss: 13.6059, Val RMSE: 3.6886\n",
      "Epoch 33/100, Train Loss: 0.3957, Train RMSE: 0.6290, Val Loss: 13.5428, Val RMSE: 3.6801\n",
      "Epoch 34/100, Train Loss: 0.3734, Train RMSE: 0.6110, Val Loss: 13.4822, Val RMSE: 3.6718\n",
      "Epoch 35/100, Train Loss: 0.3856, Train RMSE: 0.6209, Val Loss: 13.4209, Val RMSE: 3.6635\n",
      "Epoch 36/100, Train Loss: 0.3648, Train RMSE: 0.6040, Val Loss: 13.3626, Val RMSE: 3.6555\n",
      "Epoch 37/100, Train Loss: 0.3791, Train RMSE: 0.6157, Val Loss: 13.3033, Val RMSE: 3.6474\n",
      "Epoch 38/100, Train Loss: 0.3567, Train RMSE: 0.5972, Val Loss: 13.2475, Val RMSE: 3.6397\n",
      "Epoch 39/100, Train Loss: 0.3709, Train RMSE: 0.6090, Val Loss: 13.1926, Val RMSE: 3.6322\n",
      "Epoch 40/100, Train Loss: 0.3520, Train RMSE: 0.5933, Val Loss: 13.1358, Val RMSE: 3.6243\n",
      "Epoch 41/100, Train Loss: 0.3621, Train RMSE: 0.6018, Val Loss: 13.0814, Val RMSE: 3.6168\n",
      "Epoch 42/100, Train Loss: 0.3430, Train RMSE: 0.5857, Val Loss: 13.0325, Val RMSE: 3.6101\n",
      "Epoch 43/100, Train Loss: 0.3577, Train RMSE: 0.5981, Val Loss: 12.9734, Val RMSE: 3.6019\n",
      "Epoch 44/100, Train Loss: 0.3367, Train RMSE: 0.5803, Val Loss: 12.9249, Val RMSE: 3.5951\n",
      "Epoch 45/100, Train Loss: 0.3491, Train RMSE: 0.5908, Val Loss: 12.8708, Val RMSE: 3.5876\n",
      "Epoch 46/100, Train Loss: 0.3312, Train RMSE: 0.5755, Val Loss: 12.8249, Val RMSE: 3.5812\n",
      "Epoch 47/100, Train Loss: 0.3443, Train RMSE: 0.5868, Val Loss: 12.7744, Val RMSE: 3.5741\n",
      "Epoch 48/100, Train Loss: 0.3249, Train RMSE: 0.5700, Val Loss: 12.7285, Val RMSE: 3.5677\n",
      "Epoch 49/100, Train Loss: 0.3380, Train RMSE: 0.5813, Val Loss: 12.6777, Val RMSE: 3.5606\n",
      "Epoch 50/100, Train Loss: 0.3192, Train RMSE: 0.5650, Val Loss: 12.6361, Val RMSE: 3.5547\n",
      "Epoch 51/100, Train Loss: 0.3312, Train RMSE: 0.5755, Val Loss: 12.5847, Val RMSE: 3.5475\n",
      "Epoch 52/100, Train Loss: 0.3147, Train RMSE: 0.5610, Val Loss: 12.5438, Val RMSE: 3.5417\n",
      "Epoch 53/100, Train Loss: 0.3261, Train RMSE: 0.5711, Val Loss: 12.4940, Val RMSE: 3.5347\n",
      "Epoch 54/100, Train Loss: 0.3084, Train RMSE: 0.5553, Val Loss: 12.4562, Val RMSE: 3.5293\n",
      "Epoch 55/100, Train Loss: 0.3208, Train RMSE: 0.5664, Val Loss: 12.4090, Val RMSE: 3.5226\n",
      "Epoch 56/100, Train Loss: 0.3036, Train RMSE: 0.5510, Val Loss: 12.3694, Val RMSE: 3.5170\n",
      "Epoch 57/100, Train Loss: 0.3157, Train RMSE: 0.5619, Val Loss: 12.3233, Val RMSE: 3.5105\n",
      "Epoch 58/100, Train Loss: 0.2988, Train RMSE: 0.5467, Val Loss: 12.2825, Val RMSE: 3.5046\n",
      "Epoch 59/100, Train Loss: 0.3104, Train RMSE: 0.5572, Val Loss: 12.2410, Val RMSE: 3.4987\n",
      "Epoch 60/100, Train Loss: 0.2944, Train RMSE: 0.5426, Val Loss: 12.2008, Val RMSE: 3.4930\n",
      "Epoch 61/100, Train Loss: 0.3066, Train RMSE: 0.5537, Val Loss: 12.1614, Val RMSE: 3.4873\n",
      "Epoch 62/100, Train Loss: 0.2882, Train RMSE: 0.5369, Val Loss: 12.1261, Val RMSE: 3.4823\n",
      "Epoch 63/100, Train Loss: 0.3010, Train RMSE: 0.5487, Val Loss: 12.0788, Val RMSE: 3.4755\n",
      "Epoch 64/100, Train Loss: 0.2854, Train RMSE: 0.5342, Val Loss: 12.0469, Val RMSE: 3.4709\n",
      "Epoch 65/100, Train Loss: 0.2972, Train RMSE: 0.5451, Val Loss: 12.0073, Val RMSE: 3.4652\n",
      "Epoch 66/100, Train Loss: 0.2807, Train RMSE: 0.5298, Val Loss: 11.9739, Val RMSE: 3.4603\n",
      "Epoch 67/100, Train Loss: 0.2924, Train RMSE: 0.5408, Val Loss: 11.9309, Val RMSE: 3.4541\n",
      "Epoch 68/100, Train Loss: 0.2777, Train RMSE: 0.5270, Val Loss: 11.9031, Val RMSE: 3.4501\n",
      "Epoch 69/100, Train Loss: 0.2884, Train RMSE: 0.5370, Val Loss: 11.8590, Val RMSE: 3.4437\n",
      "Epoch 70/100, Train Loss: 0.2721, Train RMSE: 0.5216, Val Loss: 11.8306, Val RMSE: 3.4396\n",
      "Epoch 71/100, Train Loss: 0.2845, Train RMSE: 0.5334, Val Loss: 11.7894, Val RMSE: 3.4336\n",
      "Epoch 72/100, Train Loss: 0.2693, Train RMSE: 0.5189, Val Loss: 11.7630, Val RMSE: 3.4297\n",
      "Epoch 73/100, Train Loss: 0.2803, Train RMSE: 0.5294, Val Loss: 11.7236, Val RMSE: 3.4240\n",
      "Epoch 74/100, Train Loss: 0.2655, Train RMSE: 0.5153, Val Loss: 11.6992, Val RMSE: 3.4204\n",
      "Epoch 75/100, Train Loss: 0.2767, Train RMSE: 0.5260, Val Loss: 11.6588, Val RMSE: 3.4145\n",
      "Epoch 76/100, Train Loss: 0.2614, Train RMSE: 0.5113, Val Loss: 11.6328, Val RMSE: 3.4107\n",
      "Epoch 77/100, Train Loss: 0.2724, Train RMSE: 0.5219, Val Loss: 11.5929, Val RMSE: 3.4048\n",
      "Epoch 78/100, Train Loss: 0.2581, Train RMSE: 0.5080, Val Loss: 11.5671, Val RMSE: 3.4010\n",
      "Epoch 79/100, Train Loss: 0.2698, Train RMSE: 0.5194, Val Loss: 11.5316, Val RMSE: 3.3958\n",
      "Epoch 80/100, Train Loss: 0.2545, Train RMSE: 0.5044, Val Loss: 11.5069, Val RMSE: 3.3922\n",
      "Epoch 81/100, Train Loss: 0.2662, Train RMSE: 0.5159, Val Loss: 11.4698, Val RMSE: 3.3867\n",
      "Epoch 82/100, Train Loss: 0.2508, Train RMSE: 0.5008, Val Loss: 11.4452, Val RMSE: 3.3831\n",
      "Epoch 83/100, Train Loss: 0.2622, Train RMSE: 0.5120, Val Loss: 11.4111, Val RMSE: 3.3780\n",
      "Epoch 84/100, Train Loss: 0.2489, Train RMSE: 0.4989, Val Loss: 11.3881, Val RMSE: 3.3746\n",
      "Epoch 85/100, Train Loss: 0.2588, Train RMSE: 0.5088, Val Loss: 11.3514, Val RMSE: 3.3692\n",
      "Epoch 86/100, Train Loss: 0.2447, Train RMSE: 0.4947, Val Loss: 11.3262, Val RMSE: 3.3654\n",
      "Epoch 87/100, Train Loss: 0.2564, Train RMSE: 0.5064, Val Loss: 11.2942, Val RMSE: 3.3607\n",
      "Epoch 88/100, Train Loss: 0.2414, Train RMSE: 0.4913, Val Loss: 11.2716, Val RMSE: 3.3573\n",
      "Epoch 89/100, Train Loss: 0.2521, Train RMSE: 0.5021, Val Loss: 11.2370, Val RMSE: 3.3522\n",
      "Epoch 90/100, Train Loss: 0.2386, Train RMSE: 0.4885, Val Loss: 11.2155, Val RMSE: 3.3490\n",
      "Epoch 91/100, Train Loss: 0.2507, Train RMSE: 0.5007, Val Loss: 11.1810, Val RMSE: 3.3438\n",
      "Epoch 92/100, Train Loss: 0.2357, Train RMSE: 0.4855, Val Loss: 11.1603, Val RMSE: 3.3407\n",
      "Epoch 93/100, Train Loss: 0.2477, Train RMSE: 0.4977, Val Loss: 11.1286, Val RMSE: 3.3360\n",
      "Epoch 94/100, Train Loss: 0.2336, Train RMSE: 0.4833, Val Loss: 11.1067, Val RMSE: 3.3327\n",
      "Epoch 95/100, Train Loss: 0.2429, Train RMSE: 0.4928, Val Loss: 11.0743, Val RMSE: 3.3278\n",
      "Epoch 96/100, Train Loss: 0.2306, Train RMSE: 0.4802, Val Loss: 11.0548, Val RMSE: 3.3249\n",
      "Epoch 97/100, Train Loss: 0.2419, Train RMSE: 0.4918, Val Loss: 11.0216, Val RMSE: 3.3199\n",
      "Epoch 98/100, Train Loss: 0.2273, Train RMSE: 0.4768, Val Loss: 11.0070, Val RMSE: 3.3177\n",
      "Epoch 99/100, Train Loss: 0.2383, Train RMSE: 0.4881, Val Loss: 10.9721, Val RMSE: 3.3124\n",
      "Epoch 100/100, Train Loss: 0.2249, Train RMSE: 0.4742, Val Loss: 10.9549, Val RMSE: 3.3098\n",
      "\n",
      "Finished Training. Best Validation RMSE: 3.3098 at epoch 100\n",
      "Training and validation loop finished.\n",
      "Trained CF model saved to cf_model.pth\n",
      "CF mappings saved to cf_mappings.pkl\n",
      "--- Evaluating on Test Set ---\n",
      "--- Test Set Evaluation Finished, Average Test Loss: 10.9270, Test RMSE: 3.3056 ---\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize Model, Optimizer, Criterion ---\n",
    "\n",
    "# Ensure num_users and num_items were correctly determined in the data loading cell\n",
    "if num_users > 0 and num_items > 0:\n",
    "    cf_model = CFModel(num_users, num_items, EMBEDDING_DIM).to(device)\n",
    "    criterion = nn.MSELoss() # Mean Squared Error Loss for ratings\n",
    "    optimizer = optim.Adam(cf_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    print(\"CF model, criterion, and optimizer initialized.\")\n",
    "\n",
    "    # --- Start Training & Validation ---\n",
    "    if train_loader or val_loader: # Proceed only if there's data to train or validate\n",
    "        print(\"Starting training and validation...\")\n",
    "        train_and_validate(cf_model, train_loader, val_loader, criterion, optimizer, EPOCHS, device)\n",
    "        print(\"Training and validation loop finished.\")\n",
    "\n",
    "        # --- Save the Trained Model ---\n",
    "        try:\n",
    "            torch.save(cf_model.state_dict(), OUTPUT_MODEL_PATH)\n",
    "            print(f\"Trained CF model saved to {OUTPUT_MODEL_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving trained model: {e}\")\n",
    "\n",
    "        # --- Save the Mappings Used During Training ---\n",
    "        try:\n",
    "            mappings_to_save = {\n",
    "                'user_id_to_idx': user_id_to_idx,\n",
    "                'item_id_to_idx': item_id_to_idx\n",
    "            }\n",
    "            with open(CF_MAPPINGS_PATH, 'wb') as f:\n",
    "                pickle.dump(mappings_to_save, f)\n",
    "            print(f\"CF mappings saved to {CF_MAPPINGS_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving CF mappings: {e}\")\n",
    "\n",
    "        # --- Optional: Evaluate on Test Set ---\n",
    "        if test_loader:\n",
    "            print(\"--- Evaluating on Test Set ---\")\n",
    "            test_loss, test_rmse = evaluate_model(cf_model, test_loader, criterion, device)\n",
    "            print(f'--- Test Set Evaluation Finished, Average Test Loss: {test_loss:.4f}, Test RMSE: {test_rmse:.4f} ---')\n",
    "        else:\n",
    "            print(\"Skipping test set evaluation: test_loader not available.\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Skipping training as no training or validation data is available.\")\n",
    "else:\n",
    "    print(\"Error: Cannot initialize model. num_users or num_items is zero. Check data loading.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
